{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V58jTLJeZpk_",
        "outputId": "9a4bebc2-69d8-4969-a0f6-26a9008f7059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "from keras.layers import Embedding\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the raw data poetry of blake and print first 500 word \n",
        "book_text = nltk.corpus.gutenberg.raw('blake-poems.txt')\n",
        "print(book_text[1:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7FiUY1uEmzm",
        "outputId": "f38a56bd-7f85-4513-c781-f5b66e10dcdf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poems by William Blake 1789]\n",
            "\n",
            " \n",
            "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
            "and THE BOOK of THEL\n",
            "\n",
            "\n",
            " SONGS OF INNOCENCE\n",
            " \n",
            " \n",
            " INTRODUCTION\n",
            " \n",
            " Piping down the valleys wild,\n",
            "   Piping songs of pleasant glee,\n",
            " On a cloud I saw a child,\n",
            "   And he laughing said to me:\n",
            " \n",
            " \"Pipe a song about a Lamb!\"\n",
            "   So I piped with merry cheer.\n",
            " \"Piper, pipe that song again;\"\n",
            "   So I piped: he wept to hear.\n",
            " \n",
            " \"Drop thy pipe, thy happy pipe;\n",
            "   Sing thy songs of happy cheer:!\"\n",
            " So I sang the same again,\n",
            "   While he wept wi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(book_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PUrIh8U_bIU",
        "outputId": "f4a7b981-2df4-4646-afa0-5930baacd00c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38153"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing data\n",
        "To remove the punctuations and special characters, I define a function as you can see in below. This function get text and return clean and lower text"
      ],
      "metadata": {
        "id": "zi0G87DiEWFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1r_SDAw3dQDA"
      },
      "outputs": [],
      "source": [
        "#data preprocessing\n",
        "def text_preprocess(sen):\n",
        "  # Remove punctuations and numbers\n",
        "  sentence = re.sub(('[^a-zA-Z]'), \" \", sen)\n",
        "\n",
        "  # Single character removal\n",
        "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "  # Removing multiple spaces\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "  return sentence.lower()\n",
        "\n",
        "\n",
        "book_text = text_preprocess(book_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book_text[0:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "AJBxPxUO-F0p",
        "outputId": "899d0033-4cfc-4aad-8b06-dc670f89f578"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' poems by william blake songs of innocence and of experience and the book of thel songs of innocence introduction piping down the valleys wild piping songs of pleasant glee on cloud saw child and he laughing said to me pipe song about lamb so piped with merry cheer piper pipe that song again so pipe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# as you know DL model only accept number to train so we need to convert Text to number to be ready for training our DL model. there are different approaches for this, but here I use a simple technique, named tokenisation.\n",
        " "
      ],
      "metadata": {
        "id": "KPSuOU4ZAnCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gafdMY5jgIRY",
        "outputId": "1b7e7607-61b9-4a69-d3ba-23ff4bb8f61c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34028"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(book_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1qwZ4oqgYqY",
        "outputId": "a62da703-3e28-46eb-f8d2-32d71c8ecd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OLztkRuJkpMi"
      },
      "outputs": [],
      "source": [
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 25  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "  in_seq = book_text_words[i:i + input_seq_length]\n",
        "  input_sequence_words.append(in_seq)\n",
        "  out_seq = book_text_words[i + input_seq_length]\n",
        "  input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "  output_words.append(word_2_index[out_seq])\n",
        "\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import python libraries\n",
        "# import numpy as np\n",
        "# from tensorflow.keras.models import Sequential, load_model\n",
        "# from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from random import randint\n",
        "# import re\n",
        "\n",
        "# import nltk   # natural language tool kit library\n",
        "# nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "# from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "# print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# # get the book text\n",
        "# book_text = nltk.corpus.gutenberg.raw('blake-poems.txt')\n",
        "\n",
        "# # Data preprocessing\n",
        "# def preprocess_text(sen):\n",
        "#     # Remove punctuations and numbers\n",
        "#     sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "#     # Single character removal\n",
        "#     sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "#     # Removing multiple spaces\n",
        "#     sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "#     return sentence.lower()\n",
        "# book_text = preprocess_text(book_text)\n",
        "\n",
        "# book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# # convert words to numbers\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# nltk.download('punkt')\n",
        "# book_text_words = (word_tokenize(book_text))\n",
        "# n_words = len(book_text_words)\n",
        "# unique_words = len(set(book_text_words))\n",
        "\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# tokenizer = Tokenizer(num_words=unique_words)\n",
        "# tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "# vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "# word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# # Create the input sequences\n",
        "# input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "# input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "# output_words = []     # empty list to hold the output words\n",
        "# input_seq_length = 25  # length of the input sequence\n",
        "# for i in range(0, n_words - input_seq_length , 1):\n",
        "#     in_seq = book_text_words[i:i + input_seq_length]\n",
        "#     input_sequence_words.append(in_seq)\n",
        "#     out_seq = book_text_words[i + input_seq_length]\n",
        "#     input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "#     output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# # reshape the input sequences to be 3-dimensional\n",
        "# X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# # Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "# #X = X / float(vocab_size)\n",
        "\n",
        "# # one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "# y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPHcsr2BJe34",
        "outputId": "eae8805b-b5c4-4c5e-ff64-b1b20cae0cff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 25, 50)            75300     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 800)               2723200   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1506)              1206306   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,004,806\n",
            "Trainable params: 4,004,806\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "103/103 [==============================] - 10s 14ms/step - loss: 6.5174\n",
            "Epoch 2/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.2130\n",
            "Epoch 3/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1963\n",
            "Epoch 4/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1894\n",
            "Epoch 5/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1862\n",
            "Epoch 6/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1813\n",
            "Epoch 7/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1776\n",
            "Epoch 8/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1758\n",
            "Epoch 9/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1730\n",
            "Epoch 10/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1706\n",
            "Epoch 11/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1691\n",
            "Epoch 12/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1676\n",
            "Epoch 13/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1679\n",
            "Epoch 14/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1655\n",
            "Epoch 15/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1635\n",
            "Epoch 16/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1636\n",
            "Epoch 17/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1622\n",
            "Epoch 18/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1610\n",
            "Epoch 19/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1592\n",
            "Epoch 20/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1595\n",
            "Epoch 21/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1605\n",
            "Epoch 22/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1584\n",
            "Epoch 23/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1568\n",
            "Epoch 24/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1564\n",
            "Epoch 25/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1554\n",
            "Epoch 26/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1556\n",
            "Epoch 27/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1551\n",
            "Epoch 28/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1544\n",
            "Epoch 29/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1541\n",
            "Epoch 30/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1536\n",
            "Epoch 31/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1534\n",
            "Epoch 32/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1525\n",
            "Epoch 33/100\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 6.1518\n",
            "Epoch 34/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1514\n",
            "Epoch 35/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1505\n",
            "Epoch 36/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1510\n",
            "Epoch 37/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1502\n",
            "Epoch 38/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1494\n",
            "Epoch 39/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1500\n",
            "Epoch 40/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1492\n",
            "Epoch 41/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1480\n",
            "Epoch 42/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1478\n",
            "Epoch 43/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1479\n",
            "Epoch 44/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1479\n",
            "Epoch 45/100\n",
            "103/103 [==============================] - 1s 13ms/step - loss: 6.1474\n",
            "Epoch 46/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1462\n",
            "Epoch 47/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1463\n",
            "Epoch 48/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1471\n",
            "Epoch 49/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1471\n",
            "Epoch 50/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1448\n",
            "Epoch 51/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1457\n",
            "Epoch 52/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1454\n",
            "Epoch 53/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1448\n",
            "Epoch 54/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1444\n",
            "Epoch 55/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1440\n",
            "Epoch 56/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1445\n",
            "Epoch 57/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1439\n",
            "Epoch 58/100\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 6.1433\n",
            "Epoch 59/100\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 6.1430\n",
            "Epoch 60/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1436\n",
            "Epoch 61/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1425\n",
            "Epoch 62/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1429\n",
            "Epoch 63/100\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 6.1419\n",
            "Epoch 64/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1428\n",
            "Epoch 65/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1417\n",
            "Epoch 66/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1424\n",
            "Epoch 67/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.5292\n",
            "Epoch 68/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1472\n",
            "Epoch 69/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1442\n",
            "Epoch 70/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1435\n",
            "Epoch 71/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1430\n",
            "Epoch 72/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1422\n",
            "Epoch 73/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1426\n",
            "Epoch 74/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1420\n",
            "Epoch 75/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1411\n",
            "Epoch 76/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1415\n",
            "Epoch 77/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1416\n",
            "Epoch 78/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1426\n",
            "Epoch 79/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1412\n",
            "Epoch 80/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1413\n",
            "Epoch 81/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1413\n",
            "Epoch 82/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1418\n",
            "Epoch 83/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1416\n",
            "Epoch 84/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1411\n",
            "Epoch 85/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1404\n",
            "Epoch 86/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1407\n",
            "Epoch 87/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1398\n",
            "Epoch 88/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1407\n",
            "Epoch 89/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1398\n",
            "Epoch 90/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1401\n",
            "Epoch 91/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1396\n",
            "Epoch 92/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1406\n",
            "Epoch 93/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1401\n",
            "Epoch 94/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1396\n",
            "Epoch 95/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1398\n",
            "Epoch 96/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1388\n",
            "Epoch 97/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1394\n",
            "Epoch 98/100\n",
            "103/103 [==============================] - 1s 14ms/step - loss: 6.1391\n",
            "Epoch 99/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1386\n",
            "Epoch 100/100\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 6.1391\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9178642ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(25):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "\n",
        "\n",
        "# BLEU score\n",
        "# Make sure you are comparing like with like\n",
        "# input sequences contain the words in lists\n",
        "# join each sequence into a string so it can be compared with the final output which is a string\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of predicted words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54JiC0PdOKWr",
        "outputId": "86e687c4-1ccb-420d-c9c4-5acd8b6886fc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "of truth new born doubt is fled and clouds of reason dark disputes and artful teazing folly is an endless maze tangled roots perplex her\n",
            "Seed word sequence: of truth new born doubt is fled and clouds of reason dark disputes and artful teazing folly is an endless maze tangled roots perplex her\n",
            "Predicted words: the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "BLEU Score for predicted words: 0.3990021400109723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp5-QOuTNqZ2"
      },
      "source": [
        "# model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPPZdbGUmywH",
        "outputId": "14b93251-65f4-42ad-cc62-79f8d9771280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1506)              1206306   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,772,706\n",
            "Trainable params: 3,772,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "206/206 [==============================] - 4s 10ms/step - loss: 6.5562\n",
            "Epoch 2/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.2160\n",
            "Epoch 3/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.1924\n",
            "Epoch 4/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.1846\n",
            "Epoch 5/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 6.1820\n",
            "Epoch 6/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.1733\n",
            "Epoch 7/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.1650\n",
            "Epoch 8/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.1469\n",
            "Epoch 9/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.1025\n",
            "Epoch 10/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.0355\n",
            "Epoch 11/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 5.9362\n",
            "Epoch 12/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 5.7953\n",
            "Epoch 13/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 5.5944\n",
            "Epoch 14/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 5.3178\n",
            "Epoch 15/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 4.9722\n",
            "Epoch 16/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 4.5464\n",
            "Epoch 17/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 4.0307\n",
            "Epoch 18/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 3.4261\n",
            "Epoch 19/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.7781\n",
            "Epoch 20/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.1051\n",
            "Epoch 21/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.5166\n",
            "Epoch 22/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.9945\n",
            "Epoch 23/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6188\n",
            "Epoch 24/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3969\n",
            "Epoch 25/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.2119\n",
            "Epoch 26/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.1188\n",
            "Epoch 27/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0629\n",
            "Epoch 28/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0337\n",
            "Epoch 29/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0226\n",
            "Epoch 30/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0171\n",
            "Epoch 31/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0213\n",
            "Epoch 32/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.5608\n",
            "Epoch 33/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3336\n",
            "Epoch 34/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0660\n",
            "Epoch 35/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0300\n",
            "Epoch 36/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0157\n",
            "Epoch 37/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0136\n",
            "Epoch 38/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0068\n",
            "Epoch 39/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0052\n",
            "Epoch 40/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0043\n",
            "Epoch 41/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0037\n",
            "Epoch 42/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0033\n",
            "Epoch 43/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0029\n",
            "Epoch 44/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0026\n",
            "Epoch 45/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0023\n",
            "Epoch 46/100\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.0020\n",
            "Epoch 47/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0018\n",
            "Epoch 48/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0016\n",
            "Epoch 49/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.0015\n",
            "Epoch 50/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch 51/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch 52/100\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.0011\n",
            "Epoch 53/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 9.8449e-04\n",
            "Epoch 54/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 8.8983e-04\n",
            "Epoch 55/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 8.0396e-04\n",
            "Epoch 56/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 7.2682e-04\n",
            "Epoch 57/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.5746e-04\n",
            "Epoch 58/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 5.9413e-04\n",
            "Epoch 59/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 5.3712e-04\n",
            "Epoch 60/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 4.8532e-04\n",
            "Epoch 61/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 4.3834e-04\n",
            "Epoch 62/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 3.9600e-04\n",
            "Epoch 63/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 3.5771e-04\n",
            "Epoch 64/100\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 3.2294e-04\n",
            "Epoch 65/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.9153e-04\n",
            "Epoch 66/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.6319e-04\n",
            "Epoch 67/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.3714e-04\n",
            "Epoch 68/100\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.1396e-04\n",
            "Epoch 69/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9296e-04\n",
            "Epoch 70/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.7414e-04\n",
            "Epoch 71/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.5670e-04\n",
            "Epoch 72/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 1.4118e-04\n",
            "Epoch 73/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.2723e-04\n",
            "Epoch 74/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.1463e-04\n",
            "Epoch 75/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.0315e-04\n",
            "Epoch 76/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 9.2870e-05\n",
            "Epoch 77/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 8.3635e-05\n",
            "Epoch 78/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 7.5252e-05\n",
            "Epoch 79/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 6.7752e-05\n",
            "Epoch 80/100\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 6.0998e-05\n",
            "Epoch 81/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 5.4931e-05\n",
            "Epoch 82/100\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 4.9451e-05\n",
            "Epoch 83/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 4.4519e-05\n",
            "Epoch 84/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 4.0109e-05\n",
            "Epoch 85/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 3.6096e-05\n",
            "Epoch 86/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 3.2525e-05\n",
            "Epoch 87/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.9322e-05\n",
            "Epoch 88/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.6390e-05\n",
            "Epoch 89/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.3780e-05\n",
            "Epoch 90/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0677\n",
            "Epoch 91/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 2.6982\n",
            "Epoch 92/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.2870\n",
            "Epoch 93/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0607\n",
            "Epoch 94/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0168\n",
            "Epoch 95/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0066\n",
            "Epoch 96/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0049\n",
            "Epoch 97/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0040\n",
            "Epoch 98/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0034\n",
            "Epoch 99/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0029\n",
            "Epoch 100/100\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.0025\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9178034df0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "#model.add(LSTM(800, return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=32, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsDtvbP9nUnN",
        "outputId": "a6c39aab-a00a-4cd2-d018-9b0980cd731c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sun does arise and make happy the skies the merry bells ring to welcome the spring the skylark and thrush the birds of the\n",
            "Seed word sequence: the sun does arise and make happy the skies the merry bells ring to welcome the spring the skylark and thrush the birds of the\n",
            "Predicted words: bush sing louder around to the bells cheerful sound while our sports shall be seen on the echoing green old john with white hair does\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(25):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te52vRpzoWhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb838b5-c092-48ea-b955-bfff77615919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed word sequence: glee on cloud saw child and he laughing said to me pipe song about lamb so piped with merry cheer piper pipe that song again\n",
            "Predicted words: of happy cheer so sang the same again while he wept with joy to hear piper sit thee down and write in book that all\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Predict next 25 words\n",
        "word_sequnce = []\n",
        "for i in range(25):\n",
        "  int_sample = np.reshape(random_seq, (1,len(random_seq), 1))     # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "\n",
        "\n",
        "  predicted_word_index = model.predict(int_sample, verbose=0)   # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "  predict_word_id = np.argmax(predicted_word_index)\n",
        "  word_sequnce.append(index_2_word[predict_word_id])\n",
        "\n",
        "  random_seq.append(predict_word_id)            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "  random_seq = random_seq[1:len(random_seq)]    # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "refrence = seq\n",
        "candidate = ' '.join(word_sequnce )  # make the list of words into a string\n",
        "score = sentence_bleu(refrence, candidate )\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd08-lM8NZzb"
      },
      "source": [
        "#Model 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RanMIim6Ndz6",
        "outputId": "6a0b701e-de7b-45a4-b7ab-c05a992d8b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 389)               311589    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,877,989\n",
            "Trainable params: 2,877,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 3s 32ms/step - loss: 5.7754\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 5.2560\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 5.0284\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 4.8133\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 4.5592\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 4.2620\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 3.9642\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 3.5905\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 3.1967\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 2.7624\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 2.3346\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.9143\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.5598\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.2269\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.9603\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.7470\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.5732\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.4399\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.3437\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2670\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbba6f54450>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "#model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1RkzrfDORZC",
        "outputId": "7b82d623-1559-4ed8-ff65-e687b0abb922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no no never can it be never never can it be and can he who smiles on all hear the wren with sorrows small hear\n",
            "Seed word sequence: no no never can it be never never can it be and can he who smiles on all hear the wren with sorrows small hear \n",
            "\n",
            "Predicted words: and the and and and and in and in and in and in and in and in and in and in and in in in  \n",
            "\n",
            "BLEU Score for predicted words: 0.5566387983012375  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(25):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "\n",
        "\n",
        "# BLEU score\n",
        "# Make sure you are comparing like with like\n",
        "# input sequences contain the words in lists\n",
        "# join each sequence into a string so it can be compared with the final output which is a string\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of predicted words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s \\n'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s  \\n'%(candidate))\n",
        "print('BLEU Score for predicted words: %s  \\n'%(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw3aJac3Ts7-"
      },
      "source": [
        "# Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4G69NJYTv0J",
        "outputId": "ad5005cf-44e3-499b-ac80-526478b42497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 389)               311589    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,877,989\n",
            "Trainable params: 2,877,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 2s 34ms/step - loss: 5.7814\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 5.2441\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 5.0455\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 4.8361\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 4.6006\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 4.3144\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 3.9693\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 3.5967\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 3.1638\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 2.7261\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 2.2844\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.8638\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.4951\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.1879\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.9208\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.7254\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.5603\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.4349\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.3365\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2620\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbba6aa4510>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "#model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymzt_hRwU207",
        "outputId": "3435ad2e-d28f-4387-bb76-92cf474c44db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dost thou know who made thee gave thee life and bid thee feed by the stream and er the mead gave thee clothing of delight\n",
            "Seed word sequence: dost thou know who made thee gave thee life and bid thee feed by the stream and er the mead gave thee clothing of delight\n",
            "Predicted words: softest clothing wolly bright gave thee such tender voice making all the vales rejoice little lamb who made thee dost thou know who made thee\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(25):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUJEggWt02tr"
      },
      "outputs": [],
      "source": [
        "model.trainable = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYDk_Wc26SsS",
        "outputId": "6306f1b6-d73c-4ef1-94f8-6ca1122de1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the base model:  2\n"
          ]
        }
      ],
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(model.layers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bScbgZaW6cRQ"
      },
      "outputs": [],
      "source": [
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 3\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gs6yJg_6vQn",
        "outputId": "63912ba6-ee43-44a6-9e7c-74afb2085b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/149\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.2584\n",
            "Epoch 101/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2217\n",
            "Epoch 102/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.1523\n",
            "Epoch 103/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.1066\n",
            "Epoch 104/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0759\n",
            "Epoch 105/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0580\n",
            "Epoch 106/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0432\n",
            "Epoch 107/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0373\n",
            "Epoch 108/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0320\n",
            "Epoch 109/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0298\n",
            "Epoch 110/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0233\n",
            "Epoch 111/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0199\n",
            "Epoch 112/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0181\n",
            "Epoch 113/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0164\n",
            "Epoch 114/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0158\n",
            "Epoch 115/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0133\n",
            "Epoch 116/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0119\n",
            "Epoch 117/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0110\n",
            "Epoch 118/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0102\n",
            "Epoch 119/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0095\n",
            "Epoch 120/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0089\n",
            "Epoch 121/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0083\n",
            "Epoch 122/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0078\n",
            "Epoch 123/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0073\n",
            "Epoch 124/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0069\n",
            "Epoch 125/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0065\n",
            "Epoch 126/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0061\n",
            "Epoch 127/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0058\n",
            "Epoch 128/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0055\n",
            "Epoch 129/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0052\n",
            "Epoch 130/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0050\n",
            "Epoch 131/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0047\n",
            "Epoch 132/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0045\n",
            "Epoch 133/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0043\n",
            "Epoch 134/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0041\n",
            "Epoch 135/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0039\n",
            "Epoch 136/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0037\n",
            "Epoch 137/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0036\n",
            "Epoch 138/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0034\n",
            "Epoch 139/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0033\n",
            "Epoch 140/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0032\n",
            "Epoch 141/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0030\n",
            "Epoch 142/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0029\n",
            "Epoch 143/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0028\n",
            "Epoch 144/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0027\n",
            "Epoch 145/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0026\n",
            "Epoch 146/149\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0025\n",
            "Epoch 147/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0024\n",
            "Epoch 148/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0023\n",
            "Epoch 149/149\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0022\n"
          ]
        }
      ],
      "source": [
        "fine_tune_epochs = 50\n",
        "total_epochs =  99 + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(X,y,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=99,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D5gQkItQJic",
        "outputId": "c8469f9b-291c-4dbf-a792-8967183d48dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "little child a child and thou lamb we are called by his name little lamb god bless thee little lamb god bless thee the little\n",
            "Seed word sequence: little child a child and thou lamb we are called by his name little lamb god bless thee little lamb god bless thee the little\n",
            "Predicted words: black boy my mother bore me in the southern wild and am black but oh my soul is white white as an angel is the\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(25):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}